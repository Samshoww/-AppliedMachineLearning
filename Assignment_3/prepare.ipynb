{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\samar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\samar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\samar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Required Libraries (same as Colab)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 5572 rows and 2 columns.\n",
      "  label                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "def load_from_path(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Loads SMS spam dataset from the given file path.\"\"\"\n",
    "    # Read with header=None to prevent first row being treated as header\n",
    "    data = pd.read_csv(path, sep=',', header=None, names=['label', 'message'], encoding='latin-1')\n",
    "    \n",
    "    # Remove the first row if it contains column names\n",
    "    if data.iloc[0, 0] == 'Label' and data.iloc[0, 1] == 'Message':\n",
    "        data = data.iloc[1:].reset_index(drop=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Define dataset path\n",
    "path_sms_data = r\"C:\\Users\\samar\\OneDrive\\Documents\\AML 3\\raw_data.csv\"\n",
    "data = load_from_path(path_sms_data)\n",
    "\n",
    "# Display dataset info\n",
    "print(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoding completed. Spam Count: 747, Ham Count: 4825\n"
     ]
    }
   ],
   "source": [
    "def encode_labels(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Encodes labels: 'ham' -> 0, 'spam' -> 1.\"\"\"\n",
    "    df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "    return df\n",
    "\n",
    "data = encode_labels(data)\n",
    "print(f\"Label encoding completed. Spam Count: {sum(data['label'])}, Ham Count: {len(data) - sum(data['label'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing completed. Sample output:\n",
      "   label                                            message\n",
      "0      0  go jurong point crazy available bugis n great ...\n",
      "1      0                            ok lar joking wif u oni\n",
      "2      1  free entry wkly comp win fa cup final tkts st ...\n",
      "3      0                u dun say early hor u c already say\n",
      "4      0           nah dont think go usf life around though\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Cleans text by lowercasing, removing punctuation, numbers, and stopwords, then lemmatizing.\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # Tokenization (Using split instead of nltk.word_tokenize)\n",
    "    tokens = text.split()  # No punkt required!\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatization (Using WordNetLemmatizer)\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess_data(df: pd.DataFrame, text_col: str = 'message') -> pd.DataFrame:\n",
    "    \"\"\"Applies text preprocessing to the message column.\"\"\"\n",
    "    df[text_col] = df[text_col].apply(preprocess_text)\n",
    "    return df\n",
    "\n",
    "# Apply preprocessing\n",
    "data_preprocessed = preprocess_data(data)\n",
    "\n",
    "# Display sample output\n",
    "print(\"\\nPreprocessing completed. Sample output:\")\n",
    "print(data_preprocessed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Splitting Completed:\n",
      "  Train: 4457 rows -> ./data_splits\\train.csv\n",
      "  Validation: 557 rows -> ./data_splits\\validation.csv\n",
      "  Test: 558 rows -> ./data_splits\\test.csv\n",
      "\n",
      "Files created:\n",
      "['test.csv', 'train.csv', 'validation.csv']\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def split_and_save_data(\n",
    "    data: pd.DataFrame,\n",
    "    output_path: str = './data_splits',  # Changed to local relative path\n",
    "    train_ratio: float = 0.8,\n",
    "    val_ratio: float = 0.1,\n",
    "    label_col: str = \"label\",\n",
    "    random_state: int = 42\n",
    ") -> None:\n",
    "    \"\"\"Splits data into train, validation, and test sets and saves them as CSV files.\"\"\"\n",
    "    os.makedirs(output_path, exist_ok=True)  # Create directory if not exists\n",
    "\n",
    "    # Separate features (X) and labels (y)\n",
    "    y = data[label_col]\n",
    "    X = data.drop(columns=[label_col])\n",
    "\n",
    "    # Split Train & Temporary (Validation + Test)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=(1 - train_ratio), stratify=y, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Further split Validation & Test\n",
    "    test_ratio = 1 - train_ratio - val_ratio\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=(test_ratio / (val_ratio + test_ratio)),\n",
    "        stratify=y_temp, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Merge Labels Back\n",
    "    train_data = pd.concat([y_train, X_train], axis=1)\n",
    "    val_data = pd.concat([y_val, X_val], axis=1)\n",
    "    test_data = pd.concat([y_test, X_test], axis=1)\n",
    "\n",
    "    # Save Splits\n",
    "    train_data.to_csv(os.path.join(output_path, 'train.csv'), index=False)\n",
    "    val_data.to_csv(os.path.join(output_path, 'validation.csv'), index=False)\n",
    "    test_data.to_csv(os.path.join(output_path, 'test.csv'), index=False)\n",
    "\n",
    "    # Print Summary\n",
    "    print(\"\\nData Splitting Completed:\")\n",
    "    print(f\"  Train: {len(train_data)} rows -> {os.path.join(output_path, 'train.csv')}\")\n",
    "    print(f\"  Validation: {len(val_data)} rows -> {os.path.join(output_path, 'validation.csv')}\")\n",
    "    print(f\"  Test: {len(test_data)} rows -> {os.path.join(output_path, 'test.csv')}\")\n",
    "\n",
    "# Run Data Splitting\n",
    "split_and_save_data(data_preprocessed)\n",
    "\n",
    "# Verify files were created\n",
    "print(\"\\nFiles created:\")\n",
    "print(os.listdir('./data_splits'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
