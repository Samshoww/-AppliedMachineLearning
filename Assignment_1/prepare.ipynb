{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71w9-wHs_r5W",
        "outputId": "b79c1136-6172-4312-b486-d6e3497d07a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "import os\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up stopwords and lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "### Step 1: Load the Dataset\n",
        "def load_from_path(path: str) -> pd.DataFrame:\n",
        "    \"\"\"Loads SMS spam dataset from the given file path.\"\"\"\n",
        "    data = pd.read_csv(path, sep='\\t', names=['label', 'message'], encoding='latin-1')\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "epqNtiTNACsz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset path (\n",
        "path_sms_data = '/content/SMSSpamCollection'\n",
        "data = load_from_path(path_sms_data)"
      ],
      "metadata": {
        "id": "nwJEeMn0AHlp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display dataset shape and first few rows\n",
        "print(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6gGq_kJAQbr",
        "outputId": "68ddabe0-fd28-4ea8-dc3d-222d09038dbc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded with 5572 rows and 2 columns.\n",
            "  label                                            message\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 2: Label Encoding\n",
        "def encode_labels(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Encodes labels: 'ham' -> 0, 'spam' -> 1.\"\"\"\n",
        "    df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
        "    return df\n",
        "\n",
        "data = encode_labels(data)\n",
        "print(f\"Label encoding completed. Spam Count: {sum(data['label'])}, Ham Count: {len(data) - sum(data['label'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1PP_XX3AVBx",
        "outputId": "a12b88e2-4825-4bb9-97c9-9a59e4ac1252"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label encoding completed. Spam Count: 747, Ham Count: 4825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 3: Preprocessing Function (Without NLTK Tokenizer)\n",
        "def preprocess_text(text: str) -> str:\n",
        "    \"\"\"Cleans text by lowercasing, removing punctuation, numbers, and stopwords, then lemmatizing.\"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Remove digits\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    # Tokenization (Using split instead of nltk.word_tokenize)\n",
        "    tokens = text.split()  # No punkt required!\n",
        "\n",
        "    # Remove stopwords\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lemmatization (Using WordNetLemmatizer)\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "### Step 4: Apply Preprocessing\n",
        "def preprocess_data(df: pd.DataFrame, text_col: str = 'message') -> pd.DataFrame:\n",
        "    \"\"\"Applies text preprocessing to the message column.\"\"\"\n",
        "    df[text_col] = df[text_col].apply(preprocess_text)\n",
        "    return df\n",
        "\n",
        "# Apply preprocessing\n",
        "data_preprocessed = preprocess_data(data)\n",
        "\n",
        "# Display sample output\n",
        "print(\"\\nPreprocessing completed. Sample output:\")\n",
        "print(data_preprocessed.head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvJhYQ7OAh9x",
        "outputId": "6a9edfc5-fcde-47b8-d821-20fd758dd2d0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preprocessing completed. Sample output:\n",
            "   label                                            message\n",
            "0      0  go jurong point crazy available bugis n great ...\n",
            "1      0                            ok lar joking wif u oni\n",
            "2      1  free entry wkly comp win fa cup final tkts st ...\n",
            "3      0                u dun say early hor u c already say\n",
            "4      0           nah dont think go usf life around though\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 5: Splitting the Data and Saving Locally\n",
        "def split_and_save_data(\n",
        "    data: pd.DataFrame,\n",
        "    output_path: str = 'data_splits',\n",
        "    train_ratio: float = 0.8,\n",
        "    val_ratio: float = 0.1,\n",
        "    label_col: str = \"label\",\n",
        "    random_state: int = 42\n",
        ") -> None:\n",
        "    \"\"\"Splits data into train, validation, and test sets and saves them as CSV files.\"\"\"\n",
        "\n",
        "    os.makedirs(output_path, exist_ok=True)  # Create directory if not exists\n",
        "\n",
        "    # Separate features (X) and labels (y)\n",
        "    y = data[label_col]\n",
        "    X = data.drop(columns=[label_col])\n",
        "\n",
        "    # Split Train & Temporary (Validation + Test)\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "        X, y, test_size=(1 - train_ratio), stratify=y, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Further split Validation & Test\n",
        "    test_ratio = 1 - train_ratio - val_ratio\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_temp, y_temp, test_size=(test_ratio / (val_ratio + test_ratio)),\n",
        "        stratify=y_temp, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Merge Labels Back\n",
        "    train_data = pd.concat([y_train, X_train], axis=1)\n",
        "    val_data = pd.concat([y_val, X_val], axis=1)\n",
        "    test_data = pd.concat([y_test, X_test], axis=1)\n",
        "\n",
        "    # Save Splits\n",
        "    train_data.to_csv(os.path.join(output_path, 'train.csv'), index=False)\n",
        "    val_data.to_csv(os.path.join(output_path, 'validation.csv'), index=False)\n",
        "    test_data.to_csv(os.path.join(output_path, 'test.csv'), index=False)\n",
        "\n",
        "    # Print Summary\n",
        "    print(\"\\nData Splitting Completed:\")\n",
        "    print(f\"  Train: {len(train_data)} rows -> {os.path.join(output_path, 'train.csv')}\")\n",
        "    print(f\"  Validation: {len(val_data)} rows -> {os.path.join(output_path, 'validation.csv')}\")\n",
        "    print(f\"  Test: {len(test_data)} rows -> {os.path.join(output_path, 'test.csv')}\")\n",
        "\n",
        "# Define output directory for saving files\n",
        "output_path = '/content/data_splits'  # Change this to your desired output directory\n",
        "\n",
        "# Run Data Splitting\n",
        "split_and_save_data(data_preprocessed, output_path=output_path)\n",
        "\n",
        "print(\"\\nProcessing complete. The dataset has been cleaned, split, and saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuzwyqGwHNpK",
        "outputId": "41018092-10f5-4491-cd17-ab992949bb1c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data Splitting Completed:\n",
            "  Train: 4457 rows -> /content/data_splits/train.csv\n",
            "  Validation: 557 rows -> /content/data_splits/validation.csv\n",
            "  Test: 558 rows -> /content/data_splits/test.csv\n",
            "\n",
            "Processing complete. The dataset has been cleaned, split, and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjNH6tUdHx3C",
        "outputId": "ef50554c-704b-41db-86f2-8305b8b025b5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define your Google Drive folder path\n",
        "drive_folder = \"/content/drive/My Drive/AppliedMachineLearning/Assignment_1\"\n",
        "\n",
        "# Create the directory structure\n",
        "os.makedirs(drive_folder, exist_ok=True)\n",
        "\n",
        "print(f\"Assignment folder created at: {drive_folder}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luMCll9UIio7",
        "outputId": "f8267f34-0af6-4c33-91a8-2b3154276ff7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assignment folder created at: /content/drive/My Drive/AppliedMachineLearning/Assignment_1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Ensure 'data_splits' folder exists\n",
        "os.makedirs(\"data_splits\", exist_ok=True)\n",
        "\n",
        "# Save the train, validation, and test datasets again\n",
        "data_preprocessed.to_csv(\"data_splits/train.csv\", index=False)\n",
        "split_and_save_data(data_preprocessed, output_path=\"data_splits\")\n",
        "\n",
        "# Verify that files exist\n",
        "print(\"✅ All CSV files have been saved successfully in 'data_splits'.\")\n",
        "!ls -lh data_splits/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOfzGcXXJoT0",
        "outputId": "326551e2-cf49-4fbb-8750-0891049a128a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data Splitting Completed:\n",
            "  Train: 4457 rows -> data_splits/train.csv\n",
            "  Validation: 557 rows -> data_splits/validation.csv\n",
            "  Test: 558 rows -> data_splits/test.csv\n",
            "✅ All CSV files have been saved successfully in 'data_splits'.\n",
            "total 300K\n",
            "-rw-r--r-- 1 root root  30K Jan 30 21:38 test.csv\n",
            "-rw-r--r-- 1 root root 235K Jan 30 21:38 train.csv\n",
            "-rw-r--r-- 1 root root  31K Jan 30 21:38 validation.csv\n"
          ]
        }
      ]
    }
  ]
}